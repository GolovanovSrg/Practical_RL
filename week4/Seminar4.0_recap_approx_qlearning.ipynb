{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-13 10:27:04,759] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3b5d978da0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4Jekxhj8ktSYfwWRX/fucO6AAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3b5da32048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "l_states = InputLayer((None,) + state_dim)\n",
    "l = DenseLayer(l_states, num_units=32)\n",
    "l_qvalues = DenseLayer(l, num_units=n_actions, nonlinearity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = get_output(l_qvalues, {l_states: current_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compiling agent's \"GetQValues\" function\n",
    "get_qvalues = theano.function(inputs=[current_states], outputs=predicted_qvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]), actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict q-values for next states\n",
    "predicted_next_qvalues = get_output(l_qvalues, {l_states: next_states})\n",
    "\n",
    "\n",
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = rewards + (1 - is_end) * gamma * predicted_next_qvalues.max()\n",
    "\n",
    "#don't compute gradient over target q-values (consider constant)\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mean squared error loss function\n",
    "loss = T.mean((predicted_qvalues_for_actions - target_qvalues_for_actions)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues, trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.sgd(loss, all_weights, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states, actions, rewards,next_states, is_end], updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon = 0.25 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues(s.reshape(1, -1).astype(np.float32))[0] \n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            a = np.random.choice(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(q_values) \n",
    "        \n",
    "        \n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step(s.reshape(1, -1).astype(np.float32), [a], [r], new_s.reshape(1, -1).astype(np.float32), [done])\n",
    "        \n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: \n",
    "            break\n",
    "            \n",
    "    return total_reward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:83.490\tepsilon:0.23750\n",
      "mean reward:63.290\tepsilon:0.22562\n",
      "mean reward:13.970\tepsilon:0.21434\n",
      "mean reward:10.610\tepsilon:0.20363\n",
      "mean reward:10.760\tepsilon:0.19345\n",
      "mean reward:12.010\tepsilon:0.18377\n",
      "mean reward:21.870\tepsilon:0.17458\n",
      "mean reward:10.580\tepsilon:0.16586\n",
      "mean reward:13.210\tepsilon:0.15756\n",
      "mean reward:19.540\tepsilon:0.14968\n",
      "mean reward:10.780\tepsilon:0.14220\n",
      "mean reward:20.280\tepsilon:0.13509\n",
      "mean reward:10.170\tepsilon:0.12834\n",
      "mean reward:19.550\tepsilon:0.12192\n",
      "mean reward:10.480\tepsilon:0.11582\n",
      "mean reward:24.180\tepsilon:0.11003\n",
      "mean reward:11.790\tepsilon:0.10453\n",
      "mean reward:22.160\tepsilon:0.09930\n",
      "mean reward:11.990\tepsilon:0.09434\n",
      "mean reward:22.110\tepsilon:0.08962\n",
      "mean reward:12.580\tepsilon:0.08514\n",
      "mean reward:31.210\tepsilon:0.08088\n",
      "mean reward:18.930\tepsilon:0.07684\n",
      "mean reward:24.320\tepsilon:0.07300\n",
      "mean reward:17.840\tepsilon:0.06935\n",
      "mean reward:33.760\tepsilon:0.06588\n",
      "mean reward:33.150\tepsilon:0.06259\n",
      "mean reward:48.010\tepsilon:0.05946\n",
      "mean reward:45.510\tepsilon:0.05648\n",
      "mean reward:49.380\tepsilon:0.05366\n",
      "mean reward:54.580\tepsilon:0.05098\n",
      "mean reward:48.930\tepsilon:0.04843\n",
      "mean reward:53.270\tepsilon:0.04601\n",
      "mean reward:62.430\tepsilon:0.04371\n",
      "mean reward:57.620\tepsilon:0.04152\n",
      "mean reward:62.910\tepsilon:0.03944\n",
      "mean reward:57.740\tepsilon:0.03747\n",
      "mean reward:69.330\tepsilon:0.03560\n",
      "mean reward:53.710\tepsilon:0.03382\n",
      "mean reward:110.130\tepsilon:0.03213\n",
      "mean reward:116.940\tepsilon:0.03052\n",
      "mean reward:82.250\tepsilon:0.02900\n",
      "mean reward:40.790\tepsilon:0.02755\n",
      "mean reward:51.210\tepsilon:0.02617\n",
      "mean reward:74.420\tepsilon:0.02486\n",
      "mean reward:75.700\tepsilon:0.02362\n",
      "mean reward:100.770\tepsilon:0.02244\n",
      "mean reward:163.080\tepsilon:0.02131\n",
      "mean reward:119.170\tepsilon:0.02025\n",
      "mean reward:194.670\tepsilon:0.01924\n",
      "mean reward:75.690\tepsilon:0.01827\n",
      "mean reward:60.260\tepsilon:0.01736\n",
      "mean reward:118.350\tepsilon:0.01649\n",
      "mean reward:197.970\tepsilon:0.01567\n",
      "mean reward:199.830\tepsilon:0.01488\n",
      "mean reward:200.000\tepsilon:0.01414\n",
      "mean reward:160.750\tepsilon:0.01343\n",
      "mean reward:180.370\tepsilon:0.01276\n",
      "mean reward:82.030\tepsilon:0.01212\n",
      "mean reward:112.040\tepsilon:0.01152\n",
      "mean reward:151.400\tepsilon:0.01094\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon *= 0.95\n",
    "    \n",
    "    print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon != 0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
