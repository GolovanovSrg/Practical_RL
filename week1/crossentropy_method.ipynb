{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-25 11:45:59,616] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = 1 / n_actions * np.ones((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states, actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        a = np.random.choice(n_actions, p=policy[s])\n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return states,actions,total_reward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: mean reward = -769.53700\tthreshold = -866.0\n",
      "step 1: mean reward = -757.61100\tthreshold = -857.0\n",
      "step 2: mean reward = -754.54300\tthreshold = -848.0\n",
      "step 3: mean reward = -750.21450\tthreshold = -839.0\n",
      "step 4: mean reward = -742.53650\tthreshold = -839.0\n",
      "step 5: mean reward = -730.02550\tthreshold = -830.0\n",
      "step 6: mean reward = -725.60950\tthreshold = -821.0\n",
      "step 7: mean reward = -713.89150\tthreshold = -812.0\n",
      "step 8: mean reward = -707.64750\tthreshold = -812.0\n",
      "step 9: mean reward = -700.03950\tthreshold = -794.0\n",
      "step 10: mean reward = -690.34950\tthreshold = -794.0\n",
      "step 11: mean reward = -684.78450\tthreshold = -785.0\n",
      "step 12: mean reward = -678.38250\tthreshold = -776.0\n",
      "step 13: mean reward = -666.81150\tthreshold = -776.0\n",
      "step 14: mean reward = -662.07950\tthreshold = -767.0\n",
      "step 15: mean reward = -656.63150\tthreshold = -758.0\n",
      "step 16: mean reward = -642.82900\tthreshold = -749.0\n",
      "step 17: mean reward = -639.53250\tthreshold = -740.0\n",
      "step 18: mean reward = -629.36550\tthreshold = -740.0\n",
      "step 19: mean reward = -618.89200\tthreshold = -731.0\n",
      "step 20: mean reward = -615.36950\tthreshold = -722.0\n",
      "step 21: mean reward = -602.21550\tthreshold = -713.0\n",
      "step 22: mean reward = -598.82150\tthreshold = -704.0\n",
      "step 23: mean reward = -592.90050\tthreshold = -704.0\n",
      "step 24: mean reward = -581.40700\tthreshold = -695.0\n",
      "step 25: mean reward = -576.50200\tthreshold = -686.0\n",
      "step 26: mean reward = -565.74150\tthreshold = -677.0\n",
      "step 27: mean reward = -551.55500\tthreshold = -668.0\n",
      "step 28: mean reward = -544.04500\tthreshold = -668.0\n",
      "step 29: mean reward = -537.26350\tthreshold = -650.0\n",
      "step 30: mean reward = -522.16200\tthreshold = -641.0\n",
      "step 31: mean reward = -519.80900\tthreshold = -632.0\n",
      "step 32: mean reward = -501.76900\tthreshold = -632.0\n",
      "step 33: mean reward = -500.02050\tthreshold = -623.0\n",
      "step 34: mean reward = -492.89150\tthreshold = -614.0\n",
      "step 35: mean reward = -483.36100\tthreshold = -614.0\n",
      "step 36: mean reward = -473.76900\tthreshold = -605.0\n",
      "step 37: mean reward = -471.70150\tthreshold = -596.0\n",
      "step 38: mean reward = -454.57350\tthreshold = -587.0\n",
      "step 39: mean reward = -442.05350\tthreshold = -578.0\n",
      "step 40: mean reward = -438.44300\tthreshold = -578.0\n",
      "step 41: mean reward = -432.28200\tthreshold = -569.0\n",
      "step 42: mean reward = -417.95200\tthreshold = -560.0\n",
      "step 43: mean reward = -414.54300\tthreshold = -551.0\n",
      "step 44: mean reward = -401.47350\tthreshold = -542.0\n",
      "step 45: mean reward = -397.61250\tthreshold = -533.0\n",
      "step 46: mean reward = -386.67300\tthreshold = -533.0\n",
      "step 47: mean reward = -378.03950\tthreshold = -524.0\n",
      "step 48: mean reward = -365.55700\tthreshold = -515.0\n",
      "step 49: mean reward = -356.26200\tthreshold = -506.0\n",
      "step 50: mean reward = -348.52050\tthreshold = -497.0\n",
      "step 51: mean reward = -336.82850\tthreshold = -497.0\n",
      "step 52: mean reward = -331.69800\tthreshold = -488.0\n",
      "step 53: mean reward = -319.24850\tthreshold = -479.0\n",
      "step 54: mean reward = -313.14400\tthreshold = -470.0\n",
      "step 55: mean reward = -294.33850\tthreshold = -461.0\n",
      "step 56: mean reward = -288.83300\tthreshold = -452.0\n",
      "step 57: mean reward = -281.43200\tthreshold = -452.0\n",
      "step 58: mean reward = -262.06250\tthreshold = -434.0\n",
      "step 59: mean reward = -252.07750\tthreshold = -425.0\n",
      "step 60: mean reward = -243.53650\tthreshold = -416.0\n",
      "step 61: mean reward = -227.19100\tthreshold = -398.0\n",
      "step 62: mean reward = -212.63200\tthreshold = -389.0\n",
      "step 63: mean reward = -206.21400\tthreshold = -380.0\n",
      "step 64: mean reward = -190.35050\tthreshold = -362.0\n",
      "step 65: mean reward = -178.02550\tthreshold = -335.0\n",
      "step 66: mean reward = -160.00350\tthreshold = -317.0\n",
      "step 67: mean reward = -144.69450\tthreshold = -276.0\n",
      "step 68: mean reward = -133.21950\tthreshold = -256.1\n",
      "step 69: mean reward = -122.85200\tthreshold = -226.1\n",
      "step 70: mean reward = -112.36850\tthreshold = -207.0\n",
      "step 71: mean reward = -103.59300\tthreshold = -190.0\n",
      "step 72: mean reward = -96.18600\tthreshold = -173.1\n",
      "step 73: mean reward = -85.90300\tthreshold = -158.0\n",
      "step 74: mean reward = -78.53850\tthreshold = -145.0\n",
      "step 75: mean reward = -71.20500\tthreshold = -134.1\n",
      "step 76: mean reward = -66.15150\tthreshold = -125.1\n",
      "step 77: mean reward = -60.76550\tthreshold = -112.0\n",
      "step 78: mean reward = -55.76850\tthreshold = -104.1\n",
      "step 79: mean reward = -49.18650\tthreshold = -92.0\n",
      "step 80: mean reward = -46.10250\tthreshold = -89.0\n",
      "step 81: mean reward = -41.90500\tthreshold = -82.0\n",
      "step 82: mean reward = -37.36800\tthreshold = -74.0\n",
      "step 83: mean reward = -32.60100\tthreshold = -64.0\n",
      "step 84: mean reward = -29.42400\tthreshold = -60.0\n",
      "step 85: mean reward = -26.39850\tthreshold = -55.0\n",
      "step 86: mean reward = -23.81750\tthreshold = -52.0\n",
      "step 87: mean reward = -20.52800\tthreshold = -46.0\n",
      "step 88: mean reward = -18.06200\tthreshold = -40.0\n",
      "step 89: mean reward = -15.51600\tthreshold = -36.0\n",
      "step 90: mean reward = -13.80400\tthreshold = -34.0\n",
      "step 91: mean reward = -11.77550\tthreshold = -30.0\n",
      "step 92: mean reward = -10.44100\tthreshold = -28.0\n",
      "step 93: mean reward = -8.81650\tthreshold = -26.0\n",
      "step 94: mean reward = -7.13900\tthreshold = -22.0\n",
      "step 95: mean reward = -5.63050\tthreshold = -19.0\n",
      "step 96: mean reward = -4.16050\tthreshold = -17.0\n",
      "step 97: mean reward = -2.98750\tthreshold = -15.0\n",
      "step 98: mean reward = -1.57850\tthreshold = -12.0\n",
      "step 99: mean reward = -0.92050\tthreshold = -11.0\n",
      "step 100: mean reward = -0.19850\tthreshold = -9.0\n",
      "step 101: mean reward = 0.57750\tthreshold = -8.0\n",
      "step 102: mean reward = 1.27150\tthreshold = -6.0\n",
      "step 103: mean reward = 1.83200\tthreshold = -5.0\n",
      "step 104: mean reward = 2.60600\tthreshold = -4.0\n",
      "step 105: mean reward = 2.84350\tthreshold = -4.0\n",
      "step 106: mean reward = 3.37600\tthreshold = -3.0\n",
      "step 107: mean reward = 3.79400\tthreshold = -2.0\n",
      "step 108: mean reward = 3.97450\tthreshold = -1.0\n",
      "step 109: mean reward = 4.36850\tthreshold = 0.0\n",
      "step 110: mean reward = 4.86600\tthreshold = 0.0\n",
      "step 111: mean reward = 5.07450\tthreshold = 0.0\n",
      "step 112: mean reward = 5.35300\tthreshold = 1.0\n",
      "step 113: mean reward = 5.58400\tthreshold = 1.0\n",
      "step 114: mean reward = 5.66900\tthreshold = 1.0\n",
      "step 115: mean reward = 5.89400\tthreshold = 2.0\n",
      "step 116: mean reward = 5.92300\tthreshold = 2.0\n",
      "step 117: mean reward = 6.01450\tthreshold = 2.0\n",
      "step 118: mean reward = 6.34500\tthreshold = 3.0\n",
      "step 119: mean reward = 6.50200\tthreshold = 3.0\n",
      "step 120: mean reward = 6.69250\tthreshold = 3.0\n",
      "step 121: mean reward = 6.78300\tthreshold = 3.0\n",
      "step 122: mean reward = 6.94600\tthreshold = 4.0\n",
      "step 123: mean reward = 6.77450\tthreshold = 4.0\n",
      "step 124: mean reward = 7.07650\tthreshold = 4.0\n",
      "step 125: mean reward = 6.96150\tthreshold = 4.0\n",
      "step 126: mean reward = 7.01750\tthreshold = 4.0\n",
      "step 127: mean reward = 7.05150\tthreshold = 4.0\n",
      "step 128: mean reward = 7.36250\tthreshold = 4.0\n",
      "step 129: mean reward = 7.44350\tthreshold = 4.0\n",
      "step 130: mean reward = 7.30800\tthreshold = 5.0\n",
      "step 131: mean reward = 6.91600\tthreshold = 4.0\n",
      "step 132: mean reward = 7.42800\tthreshold = 5.0\n",
      "step 133: mean reward = 7.29750\tthreshold = 5.0\n",
      "step 134: mean reward = 7.11300\tthreshold = 5.0\n",
      "step 135: mean reward = 6.99700\tthreshold = 5.0\n",
      "step 136: mean reward = 6.79000\tthreshold = 5.0\n",
      "step 137: mean reward = 6.82850\tthreshold = 5.0\n",
      "step 138: mean reward = 7.04850\tthreshold = 5.0\n",
      "step 139: mean reward = 6.84750\tthreshold = 5.0\n",
      "step 140: mean reward = 7.12600\tthreshold = 5.0\n",
      "step 141: mean reward = 7.30950\tthreshold = 5.0\n",
      "step 142: mean reward = 7.11400\tthreshold = 5.0\n",
      "step 143: mean reward = 6.99100\tthreshold = 5.0\n",
      "step 144: mean reward = 7.11800\tthreshold = 5.0\n",
      "step 145: mean reward = 7.09900\tthreshold = 5.0\n",
      "step 146: mean reward = 6.90750\tthreshold = 5.0\n",
      "step 147: mean reward = 7.45200\tthreshold = 5.0\n",
      "step 148: mean reward = 7.09700\tthreshold = 5.0\n",
      "step 149: mean reward = 7.14550\tthreshold = 5.0\n",
      "step 150: mean reward = 6.79700\tthreshold = 5.0\n",
      "step 151: mean reward = 7.27300\tthreshold = 5.0\n",
      "step 152: mean reward = 6.23150\tthreshold = 5.0\n",
      "step 153: mean reward = 7.21950\tthreshold = 5.0\n",
      "step 154: mean reward = 6.78600\tthreshold = 5.0\n",
      "step 155: mean reward = 7.17700\tthreshold = 5.0\n",
      "step 156: mean reward = 7.35950\tthreshold = 5.0\n",
      "step 157: mean reward = 7.22750\tthreshold = 5.0\n",
      "step 158: mean reward = 7.24650\tthreshold = 5.0\n",
      "step 159: mean reward = 7.08250\tthreshold = 5.0\n",
      "step 160: mean reward = 7.08450\tthreshold = 5.0\n",
      "step 161: mean reward = 7.36700\tthreshold = 5.0\n",
      "step 162: mean reward = 7.28150\tthreshold = 5.0\n",
      "step 163: mean reward = 7.12500\tthreshold = 5.0\n",
      "step 164: mean reward = 7.74800\tthreshold = 5.0\n",
      "step 165: mean reward = 7.38300\tthreshold = 5.0\n",
      "step 166: mean reward = 7.34700\tthreshold = 5.0\n",
      "step 167: mean reward = 7.45050\tthreshold = 5.0\n",
      "step 168: mean reward = 7.15400\tthreshold = 5.0\n",
      "step 169: mean reward = 7.05600\tthreshold = 5.0\n",
      "step 170: mean reward = 7.11100\tthreshold = 5.0\n",
      "step 171: mean reward = 6.96550\tthreshold = 5.0\n",
      "step 172: mean reward = 7.12050\tthreshold = 5.0\n",
      "step 173: mean reward = 7.32750\tthreshold = 5.0\n",
      "step 174: mean reward = 7.09500\tthreshold = 5.0\n",
      "step 175: mean reward = 7.08450\tthreshold = 5.0\n",
      "step 176: mean reward = 7.30700\tthreshold = 5.0\n",
      "step 177: mean reward = 7.33800\tthreshold = 5.0\n",
      "step 178: mean reward = 6.62550\tthreshold = 5.0\n",
      "step 179: mean reward = 6.64750\tthreshold = 5.0\n",
      "step 180: mean reward = 7.43600\tthreshold = 5.0\n",
      "step 181: mean reward = 7.39050\tthreshold = 5.0\n",
      "step 182: mean reward = 6.85750\tthreshold = 5.0\n",
      "step 183: mean reward = 6.99600\tthreshold = 5.0\n",
      "step 184: mean reward = 7.16000\tthreshold = 5.0\n",
      "step 185: mean reward = 7.09100\tthreshold = 5.0\n",
      "step 186: mean reward = 7.45200\tthreshold = 5.0\n",
      "step 187: mean reward = 7.00000\tthreshold = 5.0\n",
      "step 188: mean reward = 7.14750\tthreshold = 5.0\n",
      "step 189: mean reward = 7.31750\tthreshold = 5.0\n",
      "step 190: mean reward = 7.18250\tthreshold = 5.0\n",
      "step 191: mean reward = 7.55800\tthreshold = 5.0\n",
      "step 192: mean reward = 6.61900\tthreshold = 5.0\n",
      "step 193: mean reward = 7.05900\tthreshold = 5.0\n",
      "step 194: mean reward = 7.28900\tthreshold = 5.0\n",
      "step 195: mean reward = 7.52850\tthreshold = 5.0\n",
      "step 196: mean reward = 7.70100\tthreshold = 5.0\n",
      "step 197: mean reward = 7.23750\tthreshold = 5.0\n",
      "step 198: mean reward = 7.22400\tthreshold = 5.0\n",
      "step 199: mean reward = 7.35650\tthreshold = 5.0\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n_samples = 2000  #sample this many samples\n",
    "percentile = 10  #take this percent of session with highest rewards\n",
    "smoothing = 1e-6  #add this thing to all counts for stability\n",
    "plot_dist = False\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states, batch_actions, batch_rewards = map(np.array, zip(*sessions))\n",
    "\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "    \n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    \n",
    "    elite_states = batch_states[batch_rewards >= threshold]\n",
    "    elite_actions = batch_actions[batch_rewards >= threshold]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate, [elite_states, elite_actions])\n",
    "    #hint on task above: use np.percentile and numpy-style indexing\n",
    "    \n",
    "    #count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy) + smoothing\n",
    "    \n",
    "    for j in range(len(elite_states)):\n",
    "        elite_counts[elite_states[j], elite_actions[j]] += 1\n",
    "\n",
    "    a = 0.6\n",
    "    policy = (1 - a) * policy + a * elite_counts / elite_counts.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    print(\"step %i: mean reward = %.5f\\tthreshold = %.1f\"%(i, np.mean(batch_rewards), threshold))\n",
    "    \n",
    "    if plot_dist:\n",
    "        sns.distplot(batch_rewards)\n",
    "        plt.xlabel('reward')\n",
    "        plt.plot(threshold, 0, 'ro')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAESCAYAAAAR2wXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f952806d080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(batch_rewards)\n",
    "plt.xlabel('reward')\n",
    "plt.plot(threshold, 0, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-25 10:01:00,582] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa9cc0daa58>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNeb/AXN6Nx0BZNWHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9cc1c6128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-25 10:01:05,455] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/golovanov/repo/git/Practical_RL/week1/videos')\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(32, 16, 16),\n",
    "                      activation='relu',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()] * n_actions, range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0]\n",
    "        a = np.random.choice(n_actions, p=probs)\n",
    "        \n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: \n",
    "            break\n",
    "            \n",
    "    return states, actions, total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: mean reward = 19.07000\tthreshold = 17.0\n",
      "step 1: mean reward = 20.56000\tthreshold = 17.0\n",
      "step 2: mean reward = 20.16000\tthreshold = 18.0\n",
      "step 3: mean reward = 24.64000\tthreshold = 18.0\n",
      "step 4: mean reward = 25.58000\tthreshold = 18.0\n",
      "step 5: mean reward = 24.84000\tthreshold = 19.0\n",
      "step 6: mean reward = 32.14000\tthreshold = 21.0\n",
      "step 7: mean reward = 26.14000\tthreshold = 20.0\n",
      "step 8: mean reward = 28.06000\tthreshold = 22.0\n",
      "step 9: mean reward = 29.40000\tthreshold = 23.0\n",
      "step 10: mean reward = 42.32000\tthreshold = 25.0\n",
      "step 11: mean reward = 49.20000\tthreshold = 25.5\n",
      "step 12: mean reward = 38.07000\tthreshold = 29.0\n",
      "step 13: mean reward = 41.49000\tthreshold = 33.0\n",
      "step 14: mean reward = 50.61000\tthreshold = 39.0\n",
      "step 15: mean reward = 60.22000\tthreshold = 40.0\n",
      "step 16: mean reward = 54.53000\tthreshold = 40.0\n",
      "step 17: mean reward = 80.18000\tthreshold = 45.5\n",
      "step 18: mean reward = 79.54000\tthreshold = 49.0\n",
      "step 19: mean reward = 88.09000\tthreshold = 59.0\n",
      "step 20: mean reward = 81.67000\tthreshold = 61.0\n",
      "step 21: mean reward = 91.58000\tthreshold = 72.0\n",
      "step 22: mean reward = 112.88000\tthreshold = 81.0\n",
      "step 23: mean reward = 100.97000\tthreshold = 88.0\n",
      "step 24: mean reward = 117.73000\tthreshold = 93.0\n",
      "step 25: mean reward = 139.44000\tthreshold = 106.0\n",
      "step 26: mean reward = 144.05000\tthreshold = 117.0\n",
      "step 27: mean reward = 154.35000\tthreshold = 133.0\n",
      "step 28: mean reward = 176.91000\tthreshold = 157.0\n",
      "step 29: mean reward = 162.61000\tthreshold = 166.0\n",
      "step 30: mean reward = 164.44000\tthreshold = 171.0\n",
      "step 31: mean reward = 172.92000\tthreshold = 186.5\n",
      "step 32: mean reward = 165.94000\tthreshold = 196.0\n",
      "step 33: mean reward = 186.30000\tthreshold = 198.0\n",
      "step 34: mean reward = 196.52000\tthreshold = 200.0\n",
      "step 35: mean reward = 194.20000\tthreshold = 200.0\n",
      "step 36: mean reward = 192.15000\tthreshold = 200.0\n",
      "step 37: mean reward = 195.32000\tthreshold = 200.0\n",
      "step 38: mean reward = 200.00000\tthreshold = 200.0\n",
      "step 39: mean reward = 198.14000\tthreshold = 200.0\n",
      "step 40: mean reward = 197.56000\tthreshold = 200.0\n",
      "step 41: mean reward = 192.00000\tthreshold = 200.0\n",
      "step 42: mean reward = 188.40000\tthreshold = 200.0\n",
      "step 43: mean reward = 192.29000\tthreshold = 200.0\n",
      "step 44: mean reward = 198.24000\tthreshold = 200.0\n",
      "step 45: mean reward = 195.92000\tthreshold = 200.0\n",
      "step 46: mean reward = 189.38000\tthreshold = 200.0\n",
      "step 47: mean reward = 199.92000\tthreshold = 200.0\n",
      "step 48: mean reward = 200.00000\tthreshold = 200.0\n",
      "step 49: mean reward = 200.00000\tthreshold = 200.0\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from collections import deque\n",
    "from functools import reduce\n",
    "\n",
    "n_samples = 100\n",
    "percentile = 50\n",
    "\n",
    "history_size = 5\n",
    "history_states = deque(maxlen=history_size)\n",
    "history_actions = deque(maxlen=history_size)\n",
    "history_rewards = deque(maxlen=history_size)\n",
    "\n",
    "for i in range(50):\n",
    "    sessions = Parallel(n_jobs=4)(delayed(generate_session)() for _ in range(n_samples))\n",
    "\n",
    "    batch_states, batch_actions, batch_rewards = zip(*sessions)\n",
    "    \n",
    "    history_states.append(batch_states)\n",
    "    history_actions.append(batch_actions)\n",
    "    history_rewards.append(batch_rewards)\n",
    "\n",
    "    states = np.array(reduce(lambda x, y: tuple(x) + tuple(y), history_states))\n",
    "    actions = np.array(reduce(lambda x, y: tuple(x) + tuple(y), history_actions))\n",
    "    rewards = np.concatenate(history_rewards)\n",
    "\n",
    "    threshold = np.percentile(rewards, percentile)\n",
    "    \n",
    "    elite_states = states[rewards >= threshold]\n",
    "    elite_actions = actions[rewards >= threshold]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate, [elite_states, elite_actions])\n",
    "    \n",
    "    agent.fit(elite_states, elite_actions)\n",
    "\n",
    "    print(\"step %i: mean reward = %.5f\\tthreshold = %.1f\"%(i, np.mean(batch_rewards), threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Залил на gym: https://gym.openai.com/evaluations/eval_Z6aBP14TSKjnNL4TVM4iA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-25 11:46:07,555] Making new env: LunarLander-v2\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: mean reward = -207.85678\tthreshold = -172.7\n",
      "step 1: mean reward = -210.41084\tthreshold = -175.1\n",
      "step 2: mean reward = -184.94995\tthreshold = -171.8\n",
      "step 3: mean reward = -172.79960\tthreshold = -168.1\n",
      "step 4: mean reward = -143.52976\tthreshold = -157.0\n",
      "step 5: mean reward = -146.76919\tthreshold = -146.5\n",
      "step 6: mean reward = -148.23316\tthreshold = -141.2\n",
      "step 7: mean reward = -139.13473\tthreshold = -137.9\n",
      "step 8: mean reward = -130.68527\tthreshold = -131.0\n",
      "step 9: mean reward = -121.67684\tthreshold = -122.5\n",
      "step 10: mean reward = -112.80159\tthreshold = -117.0\n",
      "step 11: mean reward = -109.54837\tthreshold = -110.4\n",
      "step 12: mean reward = -103.30286\tthreshold = -106.8\n",
      "step 13: mean reward = -98.51816\tthreshold = -104.3\n",
      "step 14: mean reward = -100.94321\tthreshold = -101.0\n",
      "step 15: mean reward = -95.39076\tthreshold = -96.0\n",
      "step 16: mean reward = -86.63914\tthreshold = -92.1\n",
      "step 17: mean reward = -84.05736\tthreshold = -86.5\n",
      "step 18: mean reward = -78.29294\tthreshold = -82.4\n",
      "step 19: mean reward = -68.51719\tthreshold = -77.0\n",
      "step 20: mean reward = -74.49924\tthreshold = -74.7\n",
      "step 21: mean reward = -70.15961\tthreshold = -72.2\n",
      "step 22: mean reward = -65.44021\tthreshold = -69.7\n",
      "step 23: mean reward = -60.34129\tthreshold = -65.3\n",
      "step 24: mean reward = -53.59463\tthreshold = -62.2\n",
      "step 25: mean reward = -52.25384\tthreshold = -59.0\n",
      "step 26: mean reward = -50.00228\tthreshold = -53.4\n",
      "step 27: mean reward = -40.76689\tthreshold = -48.2\n",
      "step 28: mean reward = -40.33514\tthreshold = -46.0\n",
      "step 29: mean reward = -38.51076\tthreshold = -43.4\n",
      "step 30: mean reward = -31.42322\tthreshold = -38.3\n",
      "step 31: mean reward = -31.57094\tthreshold = -35.4\n",
      "step 32: mean reward = -22.44182\tthreshold = -33.0\n",
      "step 33: mean reward = -22.97669\tthreshold = -29.8\n",
      "step 34: mean reward = -23.39581\tthreshold = -24.5\n",
      "step 35: mean reward = -24.47848\tthreshold = -24.5\n",
      "step 36: mean reward = -15.08461\tthreshold = -23.6\n",
      "step 37: mean reward = -21.73655\tthreshold = -23.1\n",
      "step 38: mean reward = -19.32181\tthreshold = -21.0\n",
      "step 39: mean reward = -17.48085\tthreshold = -19.9\n",
      "step 40: mean reward = -13.65769\tthreshold = -17.1\n",
      "step 41: mean reward = -15.63262\tthreshold = -15.9\n",
      "step 42: mean reward = -5.81814\tthreshold = -13.5\n",
      "step 43: mean reward = -12.67264\tthreshold = -13.8\n",
      "step 44: mean reward = -13.46692\tthreshold = -11.4\n",
      "step 45: mean reward = -9.72841\tthreshold = -11.7\n",
      "step 46: mean reward = -3.47399\tthreshold = -11.3\n",
      "step 47: mean reward = -5.74748\tthreshold = -9.8\n",
      "step 48: mean reward = -2.12726\tthreshold = -7.3\n",
      "step 49: mean reward = -0.44126\tthreshold = -4.7\n",
      "step 50: mean reward = 1.56246\tthreshold = -4.5\n",
      "step 51: mean reward = 7.54401\tthreshold = -3.6\n",
      "step 52: mean reward = -4.92300\tthreshold = -1.1\n",
      "step 53: mean reward = 29.87589\tthreshold = 3.0\n",
      "step 54: mean reward = 26.97278\tthreshold = 7.4\n",
      "step 55: mean reward = 21.12261\tthreshold = 8.9\n",
      "step 56: mean reward = 23.16428\tthreshold = 8.6\n",
      "step 57: mean reward = 28.78657\tthreshold = 9.6\n",
      "step 58: mean reward = 21.22237\tthreshold = 11.2\n",
      "step 59: mean reward = 32.00852\tthreshold = 12.4\n",
      "step 60: mean reward = 39.90884\tthreshold = 20.8\n",
      "step 61: mean reward = 34.02231\tthreshold = 24.3\n",
      "step 62: mean reward = 31.01154\tthreshold = 28.8\n",
      "step 63: mean reward = 46.27986\tthreshold = 32.2\n",
      "step 64: mean reward = 61.94542\tthreshold = 57.8\n",
      "step 65: mean reward = 52.17654\tthreshold = 65.2\n",
      "step 66: mean reward = 38.71970\tthreshold = 65.8\n",
      "step 67: mean reward = 51.67568\tthreshold = 62.9\n",
      "step 68: mean reward = 43.74283\tthreshold = 63.4\n",
      "step 69: mean reward = 50.60460\tthreshold = 64.4\n",
      "step 70: mean reward = 69.18507\tthreshold = 71.4\n",
      "step 71: mean reward = 59.31935\tthreshold = 73.7\n",
      "step 72: mean reward = 64.71535\tthreshold = 83.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-043c1f993066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS8z/B4uwYivP5RQnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b497d7390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                      activation='relu',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()] * n_actions, range(n_actions));\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0]\n",
    "        a = np.random.choice(n_actions, p=probs)\n",
    "        \n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: \n",
    "            break\n",
    "            \n",
    "    return states, actions, total_reward\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from collections import deque\n",
    "from functools import reduce\n",
    "\n",
    "n_samples = 200\n",
    "percentile = 50\n",
    "\n",
    "history_size = 3\n",
    "history_states = deque(maxlen=history_size)\n",
    "history_actions = deque(maxlen=history_size)\n",
    "history_rewards = deque(maxlen=history_size)\n",
    "\n",
    "for i in range(100):\n",
    "    sessions = Parallel(n_jobs=4)(delayed(generate_session)() for _ in range(n_samples))\n",
    "\n",
    "    batch_states, batch_actions, batch_rewards = zip(*sessions)\n",
    "\n",
    "    history_states.append(batch_states)\n",
    "    history_actions.append(batch_actions)\n",
    "    history_rewards.append(batch_rewards)\n",
    "\n",
    "    states = np.array(reduce(lambda x, y: tuple(x) + tuple(y), history_states))\n",
    "    actions = np.array(reduce(lambda x, y: tuple(x) + tuple(y), history_actions))\n",
    "    rewards = np.concatenate(history_rewards)\n",
    "\n",
    "    threshold = np.percentile(rewards, percentile)\n",
    "    \n",
    "    elite_states = states[rewards >= threshold]\n",
    "    elite_actions = actions[rewards >= threshold]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate, [elite_states, elite_actions])\n",
    "    \n",
    "    agent.fit(elite_states, elite_actions)\n",
    "\n",
    "    print(\"step %i: mean reward = %.5f\\tthreshold = %.1f\"%(i, np.mean(batch_rewards), threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Залил на gym: https://gym.openai.com/evaluations/eval_OI5rn0qZQr2q3kTyYjm8tA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-25 12:24:28,832] Clearing 12 monitor files from previous run (because force=True was provided)\n",
      "[2017-02-25 12:24:28,837] Starting new video recorder writing to /home/golovanov/repo/git/Practical_RL/week1/videos/openaigym.video.0.18827.video000000.mp4\n",
      "[2017-02-25 12:24:49,293] Starting new video recorder writing to /home/golovanov/repo/git/Practical_RL/week1/videos/openaigym.video.0.18827.video000001.mp4\n",
      "[2017-02-25 12:25:02,331] Starting new video recorder writing to /home/golovanov/repo/git/Practical_RL/week1/videos/openaigym.video.0.18827.video000008.mp4\n",
      "[2017-02-25 12:25:39,589] Starting new video recorder writing to /home/golovanov/repo/git/Practical_RL/week1/videos/openaigym.video.0.18827.video000027.mp4\n",
      "[2017-02-25 12:26:35,690] Starting new video recorder writing to /home/golovanov/repo/git/Practical_RL/week1/videos/openaigym.video.0.18827.video000064.mp4\n",
      "[2017-02-25 12:27:27,348] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/golovanov/repo/git/Practical_RL/week1/videos')\n",
      "[2017-02-25 12:27:27,350] [LunarLander-v2] Uploading 100 episodes of training data\n",
      "[2017-02-25 12:27:30,388] [LunarLander-v2] Uploading videos of 5 training episodes (675347 bytes)\n",
      "[2017-02-25 12:27:33,669] [LunarLander-v2] Creating evaluation object from ./videos/ with learning curve and training video\n",
      "[2017-02-25 12:27:34,180] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on LunarLander-v2 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_OI5rn0qZQr2q3kTyYjm8tA\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "gym.upload(\"./videos/\",api_key=\"sk_T2P9FKPSSLSzdYbs1KVbvA\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part I\n",
    "\n",
    "### Tabular correntropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -10k to aroung -500 score (+- 500) and stays there. This is in part because taxi-v2 has some hard-coded randomness in the environment. Other reason is that the percentile was chosen poorly.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (1 pt) Modify the tabular CEM (CrossEntropyMethod) code to plot distribution of rewards and threshold on each tick.\n",
    "- __1.2__ (2 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples.\n",
    "\n",
    "```При увеличении перцентили, распределение быстрее смещается в положителную сторону, но при этом оно раздваивается на два пика. Один ближе к 0, другой (чем выше перцентиль, тем больше пик), - уходит в минус. Фактически, чем ниже перцентиль - тем медленнее и надежнее сходится алгоритм. И чем выше перцентиль, тем вероятнее мы выбираем \"удачливые\" стратегии, которым потом не везет. Увеличение числа сессий снижает этот недостаток на высоких перцентилях.```\n",
    "\n",
    "\n",
    "- __1.3__ (2 pts) Tune the algorithm to end up with positive average score.\n",
    "- __1.4 bonus__ (1 pt) Try to achieve a distribution where 25% or more samples score above +9.0\n",
    "- __1.5 bonus__ (2 pts) Solve and upload [Taxi-v1](https://gym.openai.com/envs/Taxi-v1) to the openai gym.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ Go to the [gym site](http://gym.openai.com/), register and obtain __api key__.\n",
    "* __2.2__ (1 pt) Upload your result to gym via gym.upload (see Results tab above, the line you need is commented)\n",
    "* __2.3__ (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2 (or both) and solve it.\n",
    "  * For MountainCar, learn to finish it in __less than 180 steps__\n",
    "  * For LunarLander, learn to get reward of __at least +50__\n",
    "  * See the tips section below, it's kinda important.\n",
    "  \n",
    "  \n",
    "* __2.4__ (1+ pt) Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "* __Please upload the results to openai gym and send links to all submissions in the e-mail__\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "* __2.5 bonus__ Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation_)\n",
    "\n",
    "* __2.6 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * [MountainCarContinuous-v0](https://gym.openai.com/envs/MountainCarContinuous-v0), [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) (4+ points if it works)\n",
    "  \n",
    "* __2.7 bonus__ Use any deep learning framework of your choice to implement policy-gradient (see lectures) on any of those envs (4 +1 per env):\n",
    "  * CartPole-v0\n",
    "  * MountainCar-v0\n",
    "  * LunarLander-v2\n",
    "  * See __tips on policy gradient__ below.\n",
    "  \n",
    "\n",
    "* __2.8 bonus__ take your favorite deep learning framework and try to get above random in [Atari Breakout](https://gym.openai.com/envs/Breakout-v0) with crossentropy method over a convolutional network.\n",
    "  * Expect at least +10 points if you get this up and running, no deadlines apply ! \n",
    "  * __See tips below on where to start, they're cruicially important__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on policy gradient\n",
    "\n",
    "* The loss function is very similar to crossentropy method. You can get away with using rewards as  __sample_weights__.\n",
    "* If your algorithm converges to a poor strategy, try regularizing with entropy or just somehow prevent agent from picking actions deterministically (e.g. when probs = 0,0,1,0,0)\n",
    "* We will use `lasagne` later in the course so you can try to [learn it](http://lasagne.readthedocs.io/en/latest/user/tutorial.html).\n",
    "* If you don't want to mess with theano just yet, try [keras](https://keras.io/getting-started/sequential-model-guide/) or [mxnet](http://mxnet.io/tutorials/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tips on atari breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's all the pre-processing and tuning done for you in the code below\n",
    "* Once you got it working, it's probably a good idea to pre-train with autoencoder or something\n",
    "* We use last 4 frames as observations to account for ball velocity\n",
    "* The code below requires ```pip install Image``` and ```pip install gym[atari]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from breakout import make_breakout\n",
    "\n",
    "env = make_breakout()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the initial state\n",
    "s = env.reset()\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plot first observation. Only one frame\n",
    "plt.imshow(s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#next frame\n",
    "new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#after 10 frames\n",
    "for _ in range(10):\n",
    "    new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "< tons of your code here or elsewhere >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
