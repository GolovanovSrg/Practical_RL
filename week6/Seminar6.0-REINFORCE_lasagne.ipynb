{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in lasagne (3 pts)\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a lasagne network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from Seminar4.0, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-01 04:48:42,772] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f05940547f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCquIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05968f3cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = env.env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "[2017-04-01 04:48:44,058] The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GT 730M (CNMeM is enabled with initial size: 70.0% of memory, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "cumulative_rewards = T.vector(\"R[batch] = r + gamma*r' + gamma^2*r'' + ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import rectify, softmax\n",
    "\n",
    "#input layer\n",
    "l_states = InputLayer((None,)+state_dim,input_var=states)\n",
    "layer = DenseLayer(l_states, num_units=128, nonlinearity=rectify)\n",
    "layer = DenseLayer(layer, num_units=64, nonlinearity=rectify)\n",
    "\n",
    "#output layer\n",
    "#this time we need to predict action probabilities,\n",
    "#so make sure your nonlinearity forces p>0 and sum_p = 1\n",
    "l_action_probas = DenseLayer(layer,\n",
    "                             num_units=n_actions,\n",
    "                             nonlinearity=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get probabilities of actions\n",
    "predicted_probas = get_output(l_action_probas)\n",
    "\n",
    "#predict action probability given state\n",
    "#if you use float32, set allow_input_downcast=True\n",
    "predict_proba = theano.function(inputs=[states], outputs=predicted_probas, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "The objective function can be defined thusly:\n",
    "\n",
    "$$ J \\approx \\sum  _i log \\pi_\\theta (a_i | s_i) \\cdot R(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select probabilities for chosen actions, pi(a_i|s_i)\n",
    "predicted_probas_for_actions = predicted_probas[T.arange(actions.shape[0]),actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#REINFORCE objective function\n",
    "J = T.mean(T.log(predicted_probas_for_actions) * cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_H = (predicted_probas * T.log(predicted_probas)).sum(axis=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_action_probas)\n",
    "\n",
    "#weight updates. maximize J = minimize -J\n",
    "updates = lasagne.updates.sgd(-J + 0.01 * neg_H, all_weights, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = theano.function([states,actions,cumulative_rewards],updates=updates,\n",
    "                             allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, #rewards at each step\n",
    "                           gamma = 0.99 #discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    R = [0]\n",
    "    for i in range(len(rewards)):\n",
    "        R.append(rewards[-i-1] + gamma * R[-1])\n",
    "    return R[::-1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #action probabilities array aka pi(a|s)\n",
    "        action_probas = predict_proba([s])[0] \n",
    "        \n",
    "        a = np.random.choice(n_actions, p=action_probas)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    cumulative_rewards = get_cumulative_rewards(rewards)\n",
    "    train_step(states,actions,cumulative_rewards)\n",
    "            \n",
    "    return sum(rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:19.060\n",
      "mean reward:21.340\n",
      "mean reward:21.030\n",
      "mean reward:24.870\n",
      "mean reward:24.030\n",
      "mean reward:30.170\n",
      "mean reward:30.390\n",
      "mean reward:27.220\n",
      "mean reward:27.710\n",
      "mean reward:31.690\n",
      "mean reward:29.250\n",
      "mean reward:32.700\n",
      "mean reward:33.660\n",
      "mean reward:28.540\n",
      "mean reward:32.990\n",
      "mean reward:40.680\n",
      "mean reward:35.250\n",
      "mean reward:38.310\n",
      "mean reward:39.320\n",
      "mean reward:42.230\n",
      "mean reward:40.630\n",
      "mean reward:48.180\n",
      "mean reward:49.580\n",
      "mean reward:45.810\n",
      "mean reward:53.350\n",
      "mean reward:54.650\n",
      "mean reward:59.730\n",
      "mean reward:59.370\n",
      "mean reward:71.810\n",
      "mean reward:68.370\n",
      "mean reward:71.110\n",
      "mean reward:82.910\n",
      "mean reward:91.790\n",
      "mean reward:95.400\n",
      "mean reward:105.340\n",
      "mean reward:95.530\n",
      "mean reward:122.680\n",
      "mean reward:138.010\n",
      "mean reward:161.530\n",
      "mean reward:169.360\n",
      "mean reward:184.850\n",
      "mean reward:162.140\n",
      "mean reward:207.080\n",
      "mean reward:150.410\n",
      "mean reward:148.870\n",
      "mean reward:170.290\n",
      "mean reward:146.350\n",
      "mean reward:144.800\n",
      "mean reward:139.280\n",
      "mean reward:123.860\n",
      "mean reward:142.720\n",
      "mean reward:235.020\n",
      "mean reward:292.710\n",
      "mean reward:234.600\n",
      "mean reward:318.560\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
